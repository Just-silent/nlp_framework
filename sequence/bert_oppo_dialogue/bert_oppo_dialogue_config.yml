# shared for multiple projects in this machine, raw data, read only

home:
  dir: '/home/zutnlp/workspace/zutnlp_research/'

device: 'cuda:0'

# Shared for multiple modules in the project
project:
  name: 'oppo_dialogue'
  test_name: 'Transformer_lr_0.003'
  dir:
    work: '{home.dir}/{project.name}/{project.test_name}'
  seed: 19980603

pretrained_models:
  name: 'bert_base_chinese'
  dir: '{home.dir}/common/pretrained_models/{pretrained_models.name}'
  full_finetuning: True
  is_use: False

data:
  name: 'oppo_dialogue'
  base_dir: '{home.dir}/data'
  train_path: '{data.base_dir}/{data.name}/train.txt'
  valid_path: '{data.base_dir}/{data.name}/valid.txt'
  test_path: '{data.base_dir}/{data.name}/test.txt'

  vocab_path: '{pretrained_models.dir}/vocab.txt'

  max_len: 0
  num_vocab: 20000
  num_tag: 100
  batch_size: 32
  batch_first: True

  token_pad_idx: 1
  tag_pad_idx: 1

  shuffle: True

  train_batch_size: 8
  train_shuffle: True

  test_batch_size: 1
  test_shuffle: False

# specified for specific module
model:
  name: 'emo_model'
  dim_embedding: 512
  dim_hidden: 512
  num_layer: 1
  bidirectional: True
  batch_first: True
  nlayer: 1
  lr: 1.0e-4
  is_pretrained_model: True
  dropout: 0.2
  nhead: 16
  num_encoder_layers: 12
  label_pad: True

loss:
  alpha: 1.0
  beta: 1.0e-3

evaluation:
  kind: seq
  type: micro # macro
  is_display: True

learn:
  dropout_rate: 0.1
  learning_rate: 1.0e-3
  momentum: 0.9
  weight_decay: 1.0e-5
  n_divide_display: 2
  batch_display: 0
  episode: 20
  dir:
    work: "{project.dir.work}//{model.name}"
    log: '{learn.dir.work}/log'
    data: "{learn.dir.work}/data"
    saved: '{learn.dir.work}/saved'
    result: '{learn.dir.work}/result/result.xlsx'
    summary: '{learn.dir.work}/summary'
    parameter: '{learn.dir.work}/parameter'
